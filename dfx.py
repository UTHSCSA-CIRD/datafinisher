""" Unpack and transform variables from a .CSV file generated by df.py
---------------------------------------------------------------------
usage: dfx.py [-l] [-h] [-o OUTFILE] [csvin]
    
"""


import argparse,csv,json
parser = argparse.ArgumentParser()
parser.add_argument("csvin",help="CSV input file generated by df.py")
parser.add_argument("-o","--outfile",help="CSV output file",default="")
parser.add_argument("-l","--log",help="Log verbose sql",action="store_true")
args = parser.parse_args()
dolog = args.log
from df_fn import xfieldj

testjson = """{"0": {"ix": 57016613993402840, "vf": null, "mc": "DX|PROF:NONPRIMARY", "cc": "GENERIC_KUH_DX_ID_2449", "cf": null, "lc": null, "st": "2012-07-11", "un": null, "vt": null, "tc": null, "nv": 456, "qt": null}, "1": {"ix": 9697820316663506, "vf": null, "mc": "DiagObs:PAT_ENC_DX", "cc": "GENERIC_KUH_DX_ID_78949", "cf": null, "lc": null, "st": "2012-07-11", "un": null, "vt": null, "tc": null, "nv": null, "qt": null}, "2": {"ix": 55916360278195536, "vf": null, "mc": "DiagObs:PAT_ENC_DX", "cc": "GENERIC_KUH_DX_ID_78949", "cf": null, "lc": null, "st": "2012-07-11", "un": null, "vt": null, "tc": null, "nv": 123, "qt": null}, "count": 3}"""

testargs = {
  'as_is' : {'as_is' : True}
  ,'concat_unique' : {'field':'cc'}
  ,'last_numeric':{'field':'nv','transform':lambda xx: xx.pop()}
  ,'true_false':{'field':'cc','transform':any}
  #,'':{'field':'','transform':None}
  ,'num_ix': {'field':'ix','transform':len}
  ,'any_vf': {'field':'vf','transform':any}
  ,'encdx_mc': {'field':'mc','transform':lambda xx,refval: any([kk == refval for kk in xx]),'refval':'DiagObs:PAT_ENC_DX'}
  ,'npdx_mc': {'field':'mc','transform':lambda xx,refval: any([kk == refval for kk in xx]),'refval':'DX|PROF:NONPRIMARY'}
  ,'max_st':{'field':'st','transform':max}
  ,'min_st':{'field':'st','transform':min}
  ,'first_un':{'field':'un','transform':lambda xx: [kk for kk in xx if kk is not None][0] if any(xx) else None}
  #,'':{'field':'','transform':None}
  ,}

"""
Note: the following works:

xfieldj(testjson,**testargs['match_mc'])

TODO: What if the data argument is None or not JSON?
TODO: On the df.py side, create an extra concept_cd||modifier_cd field
TODO: Iterate over a list of extractors for the same cell.
TODO: Have a list of lists of extractors and iterate over it for a line, returning the raw values for cells
      that are not JSON objects
DONE: Store the extractors in one dict per cell, with the dicts in a list with the same number of rows as 
      there are columns in the input data
TODO: Populate such a list from JSON strings in the first row of the input data and a set of rules for which
      default extractor goes with which set of conditions (and which extractors are invalid for which 
      conditions)
TODO: Generate the JSON strings in df.py by reading the data dictionary.
TODO: Figure out best way to hand over fresh output file from df.py directly to dfx.py
TODO: Start chopping out the no longer needed stuff from df.py
"""

def main(csvin):
  # read the csvin file
  myfhandle = open(csvin,'r')
  fr = csv.reader(myfhandle)
  myheader = fr.next()
  rawmeta = fr.next()
  # get the header
  # parse the first row
  # TODO: for more robustness, try json.loads and return empty string if failed
  meta = [json.loads(xx) if xx not in ('',None) else '' for xx in rawmeta]
  ncols = len(meta)
  newheader = []
  newmeta = []
  mytemplate = [] # this will be where we put arguments to xfieldj
  for ii in range(0,ncols): 
    # for all the inherently as-is fields...
    if(meta[ii] != '' and (len(meta[ii]['extractor'])>1 or meta[ii]['extractor'][0][0]!='as_is')):
      # all the below does is take the base name from the header and append the name(s) of the derived 
      # columns to it separated by a period, so 'v0t39_Bd_Ms_Indx' becomes 'v039_Bd_Ms_Indx.default' 
      # for example
      newheader.extend([(myheader[ii]+'.%s') % kk for kk in [jj[1] for jj in meta[ii]['extractor']]])
      # and this one extends a list of names of argument-sets for xfieldj
      mytemplate.extend([jj[0] for jj in meta[ii]['extractor']])
      # extend the length of newmeta by the same number of empty cells as how many new columns have
      # been added just now
      newmeta.extend([''] * (len(newheader)-len(newmeta)))
    # the below applies to all columns whether as_is or not
    # now add on the original base name for that series of columns
    newheader.append(myheader[ii])
    # and an as_is so the meta field gets preserved in the output
    mytemplate.append('as_is')
    # append the old meta to the newmeta
    newmeta.append(meta[ii])
  # iterate over the other rows and process them, using the respective values in mytemplate
  # TODO: update testargs with the intended values for: as_is, concat_unique, last_numeric, last_unique
  #       true_false, true_false_active
  # TODO: actually start processing the rows!
  # TODO: multiple extractors for one field
  # DONE: create the new first row for the output, with the non-meta columns having null values or something
  row0 = fr.next()
  # create an object with args for each column and iterate over it
  # write the processed rows to the outfile
  import pdb; pdb.set_trace()

if __name__ == '__main__':
    outfile = args.outfile
    # TODO: fix this path-unaware file name generator
    if outfile=="":
      outfile = "data_"+args.csvin
    main(args.csvin)
