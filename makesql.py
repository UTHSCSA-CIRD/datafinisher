""" Generate dynamic data extraction SQL for DataBuilder output files
---------------------------------------------------------------------
    
 Usage:
   makesql sqltemplate.sql dbname.db
"""

import sqlite3 as sq,argparse,re,csv,time

parser = argparse.ArgumentParser()
parser.add_argument("dbfile",help="SQLite file generated by DataBuilder")
parser.add_argument("-c","--cleanup",help="Restore dbfile to its vanilla, data-builder state",action="store_true")
parser.add_argument("-v","--csvfile",help="File to write output to, in addition to the tables that will get created in the dbfile. By default this is whatever was the name of the dbfile with '.csv' substituted for '.db'",default='OUTFILE')
parser.add_argument("-s","--style",help="What style to output the file, currently there are two-- concat which concatenates the code variables and simple which represents code variables as Yes/No, with the nulls represented by No. The default is concat.",default="concat",choices=['concat','simple'])
parser.add_argument("-d","--datecompress",help="Round all dates to the nearest X days, default is 1",default=1)
args = parser.parse_args()

# location of data dictionary sql file
ddsql = "sql/dd.sql"

#pull in all the SQLite script as variables from the config file
import ConfigParser
cfg = ConfigParser.RawConfigParser()
cfg.read('sqldump.cfg')
par=dict(cfg.items("Settings"))
"""Commenting out the loop below bc it doesn't work, and we don't need
to remove comments yet. Troubleshoot if this function is needed.

for p in par:
  par[p]=par[p].split("#",1)[0].strip()
globals().update(par)
"""

# this is to register a SQLite function for pulling out matching substrings (if found)
# and otherwise returning the original string. Useful for extracting ICD9, CPT, and LOINC codes
# from concept paths where they are embedded. For ICD9 the magic pattern is:
# '.*\\\\([VE0-9]{3}\.{0,1}[0-9]{0,2})\\\\.*'
def ifgrp(pattern,txt):
    rs = re.search(re.compile(pattern),txt)
    
    if rs == None:
      return txt 
    else:
      return rs.group(1)

def cleanup(cnx):
    t_drop = ['cdid','codefacts','codemodfacts','diagfacts','loincfacts','data_dictionary',
	      'fulloutput','oneperdayfacts','scaffold','unkfacts','unktemp','dfvars']
    v_drop = ['obs_all','obs_diag_active','obs_diag_inactive','obs_labs','obs_noins']
    print "Dropping views"
    for ii in v_drop:
      cnx.execute("drop view if exists "+ii)
    print "Dropping tables"
    for ii in t_drop:
      cnx.execute("drop table if exists "+ii)
      
# The rdt and rdst functions aren't exactly user-defined SQLite functions...
# They are python function that emit a string to concatenate into a larger SQL query
# and send back to SQL... because SQLite has a native julianday() function that's super
# easy to use. So, think of rdt and rdst as pseudo-UDFs
def rdt(datecol,factor):
    if factor == 1:
      return 'date('+datecol+')'
    else:
      return 'date(round(julianday('+datecol+')/'+str(factor)+')*'+str(factor)+')'
    
def rdst(factor):
    return rdt('start_date',factor)

def shortenwords(words,limit):
	""" Initialize the data, lengths, and indexes"""
	#get rid of the numeric codes
	words = re.sub('[0-9]','',words)
	wrds = words.split(); lens = map(len,wrds); idxs=range(len(lens))
	if limit >= len(words):
	  return(words)
	""" sort the indexes and lengths"""
	idxs.sort(key=lambda xx: lens[xx]); lens.sort()
	""" initialize the threshold and the vector of 'most important' words"""
	sumidx=0; keep=[]
	# turned out that checking the lengths of the lens and idxs is what it takes to avoid crashes
	while sumidx < limit and len(lens) > 0 and len(idxs) > 0:
		sumidx += lens.pop()
		keep.append(idxs.pop())
	keep.sort()
	shortened = [wrds[ii] for ii in keep]
	return " ".join(shortened)

def dropletters(intext):
	# This function shortens words by squeezing out vowels, most non-alphas, and repeating letters
	# the first regexp replaces multiple ocurrences of the same letter with one ocurrence of that letter
	# the \B matches a word boundary... so we only replace vowels from inside words, not leading lettters
	return re.sub(r"([a-z_ ])\1",r"\1",re.sub("\B[aeiouyAEIOUY]+","",re.sub("[^a-zA-Z _]"," ", intext)))


def main(cnx,fname,style,dtcp):
    start_time = time.time()
    # create a cursor, though most of the time turns out we don't need it because the connection
    # also has an execute() method.
    cur = cnx.cursor()
    # declare some custom functions to use within SQL queries (awesome!)
    cnx.create_function("grs",2,ifgrp)
    cnx.create_function("shw",2,shortenwords)
    cnx.create_function("drl",1,dropletters)
    # not quite foolproof-- still pulls in PROCID's, but in the final version we'll be filtering on this
    icd9grep = '.*\\\\([VE0-9]{3}(\\.[0-9]{0,2}){0,1})\\\\.*'
    loincgrep = '\\\\([0-9]{4,5}-[0-9])\\\\COMPONENT'
    
    # todo: make these passable via command-line argument for customizability
    binvals = ['No','Yes']
    # DONE (ticket #1): instead of relying on sqlite_denorm.sql, create the scaffold table from inside this 
    # script by putting the appropriate SQL commands into character strings and then passing those
    # strings as arguments to execute() (see below for an example of cur.execute() usage (cur just happens 
    # to be what we named the cursor object we created above, and execute() is a method that cursor objects have)
    # DONE: create an id to concept_cd mapping table (and filtering out redundant facts taken care of here)
    # TODO: parameterize the fact-filtering
    # create a log table
    cnx.execute("""create table if not exists dflog as
      select datetime() timestamp,
      'FirstEntryKey                                     ' key,
      'FirstEntryVal                                     ' val""")
    
    # certain values should not be changed after the first run
    cnx.execute("CREATE TABLE if not exists dfvars ( varname TEXT, textval TEXT, numval NUM )")
    olddtcp = cnx.execute("select numval from dfvars where varname = 'dtcp'").fetchall()
    if len(olddtcp) == 0:
      cnx.execute("insert into dfvars (varname,numval) values ('dtcp',"+str(dtcp)+")")
      cnx.commit()
      print "First run since cleanup, apparently"
    elif len(olddtcp) == 1:
      if dtcp != olddtcp:
	dtcp = olddtcp[0][0]
	print "Warning! Ignoring requested datecompress value and using previously stored value of "+str(dtcp)
	print "To get rid of it, do `python makesql.py -c dbfile`"
    else:
      print "Uh oh. Something is wrong there should not be more than one 'dtcp' entry in dfvars, debug time"
      import pdb; pdb.set_trace()    
        
    if cnx.execute("select count(*) from modifier_dimension").fetchone()[0] == 0:
      print "modifier_dimension is empty, let's fill it"
      # we load our local fallback db
      cnx.execute("attach './sql/datafinisher.db' as dfdb")
      # and copy from it into the input .db file's modifier_dimension
      cnx.execute("insert into modifier_dimension select * from dfdb.modifier_dimension")
      # and log that we did so
      cnx.execute("insert into dflog select datetime(),'insert','modifier_dimension'")
      cnx.commit()
    
    print "Creating scaffold table"
    # cur.execute("drop table if exists scaffold")
    # turns out it was not necessary to create an empty table first for scaffold-- the date problem 
    # that this was supposed to solve was being caused by something else, so here is the more concise
    # version that may also be a little faster
    cnx.execute("""create table if not exists scaffold as
    select distinct patient_num, """+rdst(dtcp)+""" start_date
    from observation_fact order by patient_num, start_date;
    """)

    cnx.execute("CREATE UNIQUE INDEX if not exists df_ix_scaffold ON scaffold (patient_num,start_date) ")

    print "Creating CDID table"
    # cnx.execute("drop table if exists cdid")
    cnx.execute(cdid_tab)
    print "Mapping concept codes in CDID"
    # diagnoses
    cnx.execute("""update cdid set cpath = grs('"""+icd9grep+"""',cpath) where ddomain like '%|DX_ID' """)
    cnx.execute("""update cdid set cpath = substr(ccd,instr(ccd,':')+1) where ddomain = 'ICD9'""")
    # LOINC
    cnx.execute("""update cdid set cpath = grs('"""+loincgrep+"""',cpath) where ddomain like '%|COMPONENT_ID' """)
    cnx.execute("""update cdid set cpath = substr(ccd,instr(ccd,':')+1) where ddomain = 'LOINC'""")
    cnx.execute("create UNIQUE INDEX if not exists df_ix_cdid ON cdid (id,cpath,ccd)")
    cnx.commit()
    # create a couple of cleaned-up views of observation_fact
    # replace most of the non-informative values with nulls, remove certain known redundant modifiers
    print "Creating obs_all and obs_noins views"
    cur.execute("drop view if exists obs_all")
    cur.execute(obs_all_v);
    cur.execute("drop view if exists obs_noins")
    # it would be better to aggregate multiple numeric values of the same fact collected on the same day by median, but alas
    # not all versions of SQLite have support for the median function
    cur.execute(obs_noins_v);
    
    print "Creating OBS_DIAG_ACTIVE view"
    cur.execute("drop view if exists obs_diag_active")
    cur.execute(obs_diag_inactive_v)
    print "Creating OBS_DIAG_INACTIVE view"
    cur.execute("drop view if exists obs_diag_inactive")
    cur.execute(obs_diag_inactive_v)
    print "Creating obs_labs view"
    cur.execute("drop view if exists obs_labs")
    cur.execute(obs_labs_v)
    
    # DONE: instead of a with-clause temp-table create a static data dictionary table
    #		var(concept_path,concept_cd,ddomain,vid) 
    # BTW, turns out this is a way to read and execute a SQL script
    # TODO: the shortened column names will go into this data dictionary table
    # DONE: create a filtered static copy of OBSERVATION_FACT with a vid column, maybe others
    # no vid column, relationship between concept_cd and id is not 1:1, so could get too big
    # will instead cross-walk the cdid table as needed
    # ...but perhaps unnecessary now that cdid table exists
    
    print "Creating DATA_DICTIONARY"
    #cur.execute("drop table if exists data_dictionary")
    with open(ddsql,'r') as ddf:
	ddcreate = ddf.read()
    cur.execute(ddcreate)
    # rather than running the same complicated select statement multiple times for each rule in data_dictionary
    # lets just run each selection criterion once and save it as a tag in the new RULE column
    print "Creating rules in DATA_DICTIONARY"
    # diagnosis
    cur.execute(dd_diag)
    # LOINC
    cur.execute(dd_loinc)
    # code-only
    cur.execute(dd_codeonly)
    # code-and-mod only
    cur.execute(dd_codemod_only)
    # of the concepts in this column, only one is recorded at a time
    cur.execute(dd_oneperday)
    cnx.commit()
    
    print "Creating dynamic SQL for CODEFACTS"
    cur.execute(codef_sel)
    codesel = cur.fetchone()[0]
    # dynamically generate the terms in the select statement
    # extract the terms that meet the above criterion
    codeqry = "create table if not exists codefacts as select scaffold.*,"+codesel+" from scaffold "
    # now dynamically generate the many, many join clauses and append them to codefacts
    # note the string replace-- cannot alias the table name in an update statement, so no dd
    cur.execute(codef_jrep)
    codeqry += " ".join([row[0] for row in cur.fetchall()])
    print "Creating CODEFACTS table"
    cur.execute(codeqry) 
    # same pattern as above, but now for facts that consist of both codes and modifiers

    print "Creating dynamic SQL for CODEMODFACTS"
    # select terms...
    cur.execute(codemod_sel)
    codemodsel = cur.fetchone()[0]
    codemodqry = "create table if not exists codemodfacts as select scaffold.*,"+codemodsel+" from scaffold "
    # ...and joins...
    cur.execute(codemod_jrep)
    codemodqry += " ".join([row[0] for row in cur.fetchall()])
    print "Creating CODEMODFACTS table"
    cur.execute(codemodqry)
    
    # DONE: cid's (column id's i.e. groups of variables that were selected together by the researcher)
    # ...cid's that have a ccd value of 1 (meaning there is only one distinct concept code per cid
    # any variable that doesn't have multiple values on the same day 
    # (except multiple instances of numeric values which get averaged)
    # these are expected to be numeric variables
    # TODO: create a column in obs_noins with a count of duplicates that got averaged, for QC
    print "Creating dynamic SQL for ONEPERDAY"
    # here are the select terms, but a little more complicated than in the above cases
    # on the fence whether to have extra column for the code
    # ','||colid||'_cd'||
    cur.execute(opd_sel)
    oneperdaysel = " ".join([row[0] for row in cur.fetchall()])
    oneperdayqry = "create table if not exists oneperdayfacts as select scaffold.*" + oneperdaysel + " from scaffold "
    # since we're doing ALL the non-aggregate columns at the same time, the above query is designed
    # to produce multiple rows, so we change the earlier pattern slightly so we can glue them all together
    # joins
    cur.execute(opd_jrep)
    oneperdayqry += " ".join([row[0] for row in cur.fetchall()])
    print "Creating ONEPERDAYFACTS table"
    cur.execute(oneperdayqry)
    # diagnoses output tables

    print "Creating dynamic SQL for DIAG"
    cur.execute(diag_sel)
    diagsel = cur.fetchone()[0]
    diagqry = "create table if not exists diagfacts as select scaffold.*,"+diagsel+" from scaffold "
    cur.execute(diag_jrep)
    diagqry += " ".join([row[0] for row in cur.fetchall()])
    print "Creating DIAGFACTS table"
    cur.execute(diagqry)
    
    # DONE: create the LOINCFACTS table which will contain: pn,sd,nval_num,units,info,and cpath as part of the colid
    print "Creating dynamic SQL for LOINC"
    loincsel = cnx.execute(loinc_sel).fetchone()[0]
    loincqry = "create table if not exists loincfacts as select scaffold.*,"+loincsel+" from scaffold "
    # okay, so the below is insane and should probably be refactored
    # We have the usual " ".join(blah blah blah) to create the join clauses
    # But the query that creates those clauses replaces all hyphens with underscores so that the
    # dynamically generated column names will be legal ones... but in one place in each subquery, 
    # there really should be a hyphen instead of an uderscore: where the cpath is matched to a 
    # LOINC code. So, on the python side, we change those and only those underscores back to hyphens
    # I know, pretty f*ck*d up, isn't it?
    loincqry += re.compile("cpath=(['][[0-9]{4,5})_").sub(r'cpath=\1-'," ".join([row[0] for row in cnx.execute(loinc_jrep).fetchall()]))
    cnx.execute(loincqry)
   
    # DONE: fallback on giant messy concatenated strings for everything else (for now)
    print "Creating dynamic SQL for UNKTEMP and UNKFACTS tables"
    cur.execute(utemp_sel)
    unkqryvars = cur.fetchone()
    unkqry0 = """create table if not exists unktemp as 
	select patient_num,"""+rdst(dtcp)+""" start_date,id
	,group_concat(distinct concept_cd||coalesce('&mod='||modifier_cd,'')||
	coalesce('&ins='||instance_num,'')||coalesce('&typ='||valtype_cd,'')||
	coalesce('&txt='||tval_char,'')||coalesce('&num='||nval_num,'')||
	coalesce('&flg='||valueflag_cd,'')||coalesce('&qty='||quantity_num,'')||
	coalesce('&unt='||units_cd,'')||coalesce('&loc='||location_cd,'')||
	coalesce('&cnf='||confidence_num,'')) megacode
	from obs_all join cdid on concept_cd = ccd
	where id in ("""+unkqryvars[2]+") group by patient_num,start_date,id"
    unkqry1 = "create table if not exists unkfacts as select scaffold.*,"+unkqryvars[0]+" from scaffold "
    unkqry1 += unkqryvars[1]
    print "Creating UNKTEMP table"
    cur.execute(unkqry0)
    print "Creating UNKFACTS table"
    cur.execute(unkqry1)

    print "Creating FULLOUTPUT table"
    # DONE: except we don't actually do it yet-- need to play with the variables and see the cleanest way to merge
    # the individual tables together
    # TODO: revise for consistent use of commas
    allsel = rdt('birth_date',dtcp)+""" birth_date, sex_cd 
      ,language_cd, race_cd, julianday(scaffold.start_date) - julianday("""+rdt('birth_date',dtcp)+") age_at_visit_days,"
    allsel += diagsel+','+loincsel+','+codesel+','+codemodsel+oneperdaysel+','+unkqryvars[0]
    allqry = "create table if not exists fulloutput as select scaffold.*,"+allsel
    allqry += """ from scaffold 
    left join diagfacts df on df.patient_num = scaffold.patient_num and df.start_date = scaffold.start_date
    left join loincfacts lf on lf.patient_num = scaffold.patient_num and lf.start_date = scaffold.start_date
    left join codefacts cf on cf.patient_num = scaffold.patient_num and cf.start_date = scaffold.start_date 
    left join codemodfacts cmf on cmf.patient_num = scaffold.patient_num and cmf.start_date = scaffold.start_date 
    left join oneperdayfacts one on one.patient_num = scaffold.patient_num and one.start_date = scaffold.start_date 
    left join unkfacts unk on unk.patient_num = scaffold.patient_num and unk.start_date = scaffold.start_date 
    left join patient_dimension pd on scaffold.patient_num = pd.patient_num
    order by patient_num, start_date"""
    cur.execute(allqry)

    print "Creating BINOUTPUT view of the result"
    binoutqry = """create view binoutput as select patient_num,start_date,birth_date,sex_cd
		   ,language_cd,race_cd,age_at_visit_days"""
    binoutqry += ","+",".join([" case when "+ii[1]+" is null then '"+binvals[0]+"' else '"+binvals[1]+\
			"' end "+ii[1] for ii in cnx.execute("pragma table_info(diagfacts)").fetchall()[2:]])
    binoutqry += ","+",".join([ii[1] for ii in cnx.execute("pragma table_info(loincfacts)").fetchall()[2:]])
    binoutqry += ","+",".join([" case when "+ii[1]+" is null then '"+binvals[0]+"' else '"+binvals[1]+\
			"' end "+ii[1] for ii in cnx.execute("pragma table_info(codefacts)").fetchall()[2:]])
    binoutqry += ","+",".join([" case when "+ii[1]+" is null then '"+binvals[0]+"' else '"+binvals[1]+\
			"' end "+ii[1] for ii in cnx.execute("pragma table_info(codemodfacts)").fetchall()[2:]])
    binoutqry += ","+",".join([ii[1] for ii in cnx.execute("pragma table_info(oneperdayfacts)").fetchall()[2:]])
    binoutqry += ","+",".join([" case when "+ii[1]+" is null then '"+binvals[0]+"' else '"+binvals[1]+\
			"' end "+ii[1] for ii in cnx.execute("pragma table_info(unkfacts)").fetchall()[2:]])
    binoutqry += " from fulloutput"
    cnx.execute("drop view if exists binoutput")
    cnx.execute(binoutqry)

    if style == 'simple':
      finalview = 'binoutput'
    else:
      finalview = 'fulloutput'
      
    if fname.lower() != 'none':
      ff = open(fname,'wb')
      csv.writer(ff).writerow([ii[1] for ii in con.execute("PRAGMA table_info(fulloutput)").fetchall()])
      result = cnx.execute("select * from "+finalview).fetchall()
      with ff:
	  csv.writer(ff).writerows(result)
    # DONE: write 'select * from fulloutput' to the csvfile. Should it be passed to main as a parameter? (yes)
    # TODO: create a view that replaces the various strings with simple 1/0 values
    print("--- %s seconds ---" % (time.time() - start_time))
    import pdb; pdb.set_trace()    
        
    # Boom! We covered all the cases. Messy, but at least a start.

    # the below yeah, I guess, but there are two big and easier to implement cases to do first


    """
    The decision process
      branch node
	uses mods DONE
	  map modifiers; single column of semicolon-delimited code=mod pairs
	uses other columns?
	  UNKNOWN FALLBACK, single column DONE
	code-only DONE
	  single column of semicolon-delimited codes
      leaf node
	code only DONE
	  single 1/0 column (TODO)
	uses code and mods only DONE
	  map modifiers; single column of semicolon-delimited mods DONE-ish
	uses other columns?
	  any columns besides mods have more than one value per patient-date?
	    UNKNOWN FALLBACK, single column DONE-ish
	  otherwise
	    map modifiers; single column of semicolon-delimited mods named FOO_mod; for each additional BAR, one more column FOO_BAR DONE-ish
    
    TODO: implement a user-configurable 'rulebook' containing patterns for catching data that would otherwise fall 
    into UNKNOWN FALLBACK, and expressing in a parseable form what to do when each rule is triggered.
    DONE: The data dictionary will contain information about which built-in or user-configured rule applies for each cid
    We are probably looking at several different 'dcat' style tables, broken up by type of data
    DONE: We will iterate through the data dictionary, joining new columns to the result according to the applicable rule
    """
    
    
	
if __name__ == '__main__':
    con = sq.connect(args.dbfile)

    if args.csvfile == 'OUTFILE':
      csvfile = args.dbfile.replace(".db","")+".csv"
    else:
      csvfile = args.csvfile

    if args.datecompress == 'week':
      dtcp = 7
    elif args.datecompress == 'month':
      dtcp = 365.0/12
    else:
      dtcp = args.datecompress
      
    #import pdb; pdb.set_trace();
    #import code; code.interact(local=vars())
    if args.cleanup:
      cleanup(con)
    else:
      main(con,csvfile,args.style,dtcp)



